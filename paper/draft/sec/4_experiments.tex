\section{Experiments}

\subsection{Experimental Setup}

To validate our human user simulation framework, we implement and evaluate it in the domain of restaurant guest ordering. This domain presents an ideal testbed due to key characteristics aligning with human interaction complexities~\cite{Wang2020KddRES,Yi2024MultiTurnDialogueSurvey}. Restaurant ordering involves task complexity and ambiguity, requiring multi-turn conversations where guests navigate menu options, specify customizations, and handle clarifications~\cite{Mo2024HierTOD}. The ordering process incorporates complex state tracking as guests build orders with multiple items, each having various modifiers and customization options maintained throughout the conversation. The domain captures behavioral diversity through different foodie personas exhibiting distinct ordering styles, from methodical menu explorers to decisive quick-deciders, and varying emotional responses such as frustration with overwhelming choices or confusion about menu descriptions~\cite{Ge2024PersonaHub}. These persona-driven mood complexities, combined with the structured yet flexible ordering task, provide an excellent environment for testing our framework's ability to balance persona consistency, task completion accuracy, and realistic behavioral variation.

\paragraph{Datasets}

\begin{itemize}
    \item \textbf{Personas}: 20 diverse restaurant guest personas with distinct personality traits, communication styles, and behavioral preferences~\cite{Castricato2024PERSONA,Ahmad2025SimulatingUserDiversity}
    \item \textbf{Menu}: A comprehensive restaurant menu containing 50+ items across categories with various customization options and modifiers
    \item \textbf{Order Test Cases}: 60 test cases generated by pairing each persona with 3 different target orders of varying complexity (simple, medium, and complex orders with increasing customization levels)
\end{itemize}

\paragraph{Ordering System}
We evaluate our guest simulation by making it interact with an LLM-based ordering system (GPT-4o~\cite{Hurst2024GPT4oSystemCard,Liu2023AgentBench,Shu2024EffectiveMultiAgent}) configured with restaurant-specific instructions and menu knowledge. The ordering system greets customers, processes natural language order requests, clarifies ambiguous requests and suggests menu items, confirms order details and handles modifications, and completes transactions with order summaries, mimicking a real restaurant environment. The ordering system operates independently of our guest simulation, receiving only the conversation history and generating responses without knowledge of the testcase information or guest system's internal state.

\paragraph{Agentic Simulation Implementation}

Our implementation leverages Pydantic AI~\cite{pydanticai} with GPT-4o for structured multi-agent development with type-safe tool definitions and dynamic data dependency injection. The multi-agent implementation consists of the following key components: a \textbf{Main User Agent} that orchestrates the Guest Agent with sub-agent access tools to fetch persona information and invoke sub-agents to update order state and generate behavioral attributes for the next message in the ordering conversation; \textbf{Sub Agents} including the Order Tracking Agent and Message Attributes Generation Agent with specialized tools to update order state and generate behavioral attributes as needed by the Guest Agent; \textbf{Data Models} comprising Pydantic Basemodel classes to hold data dependencies~\cite{pydanticai_docs} defining the order state and behavioral attribute data objects; and \textbf{Conversation Management} featuring turn-limited interactions with repetition detection, tracking for tool calls, latencies, token usage, and structured conversation logging for comprehensive evaluation. This implementation was run on a local machine with a 10-core Apple-M1-Max CPU with 32GB of RAM and 3.2GHz of clock speed.

\subsection{Simulation Evaluation Metrics}

We evaluate our multi-agent framework using five quantitative metrics designed to capture different aspects of simulation quality.

\paragraph{Persona Adherence Score (PAS)}
Measures how well the user maintains their assigned persona throughout the conversation~\cite{Saggar2025ScoreBeforeSpeak,Wakaki2024ComperDial}. For each user message $i$ in a conversation with $N$ messages.

\begin{equation}
PAS = \frac{1}{N} \sum_{i=1}^{N} MS_i
\end{equation}

where the message score $MS_i$ is computed as:
\begin{equation}
MS_i = \sum_{j=1}^{4} w_j \cdot C_j
\end{equation}

with equal weights $w_j = 0.25$ for each component:
\begin{itemize}
    \item $C_1$: Exploration style match (explores vs. does not explore)
    \item $C_2$: Mood tone alignment (casual, frustrated, confused, enthusiastic)
    \item $C_3$: Task execution style match (one-by-one vs. all-at-once)
    \item $C_4$: Task completion status agreement
\end{itemize}

Each component $C_j \in \{0, 1\}$ is computed based on exact match with expected persona attributes.

\paragraph{Behavioral Variance Score (BVS)}
Captures realistic fluctuations in behavior to ensure natural human-like variations. For each behavioral dimension $d \in \{task\_execution\_style, exploration\_style, mood\_tone\}$:

\begin{equation}
TR_d = \frac{1}{M-1} \sum_{i=2}^{M} \mathbb{I}(state_i^d \neq state_{i-1}^d)
\end{equation}

where $M$ is the number of behavioral states. The average transition rate is:
\begin{equation}
TR_{avg} = \frac{TR_{task\_execution\_style} + TR_{exploration\_style} + TR_{mood\_tone}}{3}
\end{equation}

BVS uses a piecewise linear scoring function peaking at 20\% transition rate since humans typically exhibit moderate variance~\cite{Park2024GenerativeAgentSimulations} :
\begin{equation}    
BVS = \begin{cases}
\frac{TR_{avg}}{0.2} & \text{if } TR_{avg} \leq 0.2 \\
1 - \frac{TR_{avg} - 0.2}{0.8} & \text{if } TR_{avg} > 0.2
\end{cases}
\end{equation}

The range of BVS is [0, 1] where 1 is the best possible score. So we can expect robotic (too static) patterns having lower BVS scores while realistic patterns would have higher BVS scores.

\paragraph{Task Restriction Adherence (TRA)}
Evaluates accuracy in achieving the target state using F1-score with normalized task item matching~\cite{Jia2024LeveragingLLMs}. Task items are normalized by:
\begin{equation}
normalize(item) = lowercase(remove\_special(remove\_filler(item)))
\end{equation}

With $T$ = normalized target items and $C$ = normalized current state items,

\begin{equation}
Precision = \frac{|C \cap T|}{|C|}, \quad Recall = \frac{|C \cap T|}{|T|}
\end{equation}

\begin{equation}
TRA = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
\end{equation}

\paragraph{Decision Explainability Index (DEI)}
Quantifies the traceability of the agentic system's internal decisions with tool usage results~\cite{Raza2024TRiSM}.

\begin{equation}
DEI = \begin{cases}
0 & \text{No tools (no explainability)} \\
\min(0.2, \frac{ED}{N} \times 0.2) & \text{Basic tools (about 20\% explainability)} \\
\min(0.5, \frac{ED}{N} \times 0.5) & \text{Basic tools + 1 subAgent (about 50\% explainability)} \\
\min(1.0, \frac{ED}{2N}) & \text{Full system (100\% explainability)}
\end{cases}
\end{equation}

where $ED$ is the count of explained decisions (tool invocations) across $N$ messages.

\paragraph{Composite Realism and Reliability Score (CRRS)}
Provides a unified score for overall user simulation quality using universal weights~\cite{Phy2020USLH}.

\begin{equation}
CRRS = 0.25 \cdot PAS + 0.20 \cdot BVS + 0.35 \cdot TRA + 0.20 \cdot DEI
\end{equation}

These weights reflect: TRA (35\%) as the primary task metric, PAS (25\%) for persona consistency, BVS (20\%) for naturalness, and DEI (20\%) for system validation. Note that DEI naturally adjusts based on the agentic system's experimental setup.




\subsection{Ablations}

We conduct systematic ablation studies across five experimental configurations to isolate the contribution of each component. 

\textbf{Config1 - Baseline LLM}: Single LLM with all information (persona, target order, conversation history) provided directly in the prompt. No agentic decomposition or tool use.

\textbf{Config2 - User Agent Only}: Guest Agent without sub-agents. Has direct access to persona, target order and conversation history through tools but no structured state tracking or behavioral control.

\textbf{Config3 - User Agent + State Tracking (ST) Agent}: Guest Agent with Order Tracking sub-agent. Maintains structured order state but lacks explicit behavioral control.

\textbf{Config4 - User Agent + Message Attributes Generation (MAG) Agent}: Guest Agent with Message Attributes Generation sub-agent. Has behavioral control but no structured state tracking.

\textbf{Config5 - Full System}: Complete multi-agent system with both Order Tracking and Message Attributes Generation sub-agents.

% \begin{table}[h]
% \centering
% \renewcommand{\arraystretch}{1.3}
% \caption{Ablation configurations showing component availability}
% \begin{tabular}{|l|c|c|c|}
% \hline
% \textbf{Configuration} & \textbf{Order Tracking} & \textbf{Message Attributes} & \textbf{Architecture} \\
% \hline
% 1 (Baseline) & - & - & Single LLM \\
% 2 (User Agent Only) & - & - & Single Agent \\
% 3 (User Agent + ST Agents) & \checkmark & - & Two Agents \\
% 4 (User Agent + MAG Agents) & - & \checkmark & Two Agents \\
% 5 (Full System) & \checkmark & \checkmark & Three Agents \\
% \hline
% \end{tabular}
% \renewcommand{\arraystretch}{1.0}
% \label{tab:ablations}
% \end{table}

\subsection{Results}

We evaluate each configuration across five quantitative metrics defined in Section 3.3. All experiments are run with 60 test cases per configuration.



\begin{table}[h]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Evaluation Metrics Across All Configurations}
\label{tab:comprehensive_metrics}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Config} & \textbf{PAS} & \textbf{BVS} & \textbf{TRA} & \textbf{DEI} & \textbf{CRRS} \\
\midrule
1 & 0.589 & 0.218 & 0.608 & 0.000 & 0.404 \\
2 & 0.585 & 0.485 & 0.582 & 0.200 & 0.487 \\
3 & 0.554 & 0.689 & \textbf{0.785} & 0.498 & 0.651 \\
4 & \textbf{0.661} & 0.000 & 0.602 & 0.432 & 0.462 \\
5 & 0.706 & \textbf{0.839} & \textbf{0.785} & \textbf{0.994} & \textbf{0.818} \\
\bottomrule
\end{tabular}
\renewcommand{\arraystretch}{1.0}
\end{minipage}
\hfill
\begin{minipage}{0.38\textwidth}
\centering
\renewcommand{\arraystretch}{1.2}
\captionof{table}{Response Computation Costs}
\label{tab:computational_cost}
\begin{tabular}{lrr}
\toprule
\textbf{Config} & \textbf{Avg. Tokens} & \textbf{Avg. Latency(s)} \\
\midrule
1 & 6,618 & 5.08 \\
2 & 13,505 & 4.56 \\
3 & 24,580 & 36.30 \\
4 & 15,763 & 16.88 \\
5 & 14,789 & 23.16 \\
\bottomrule
\end{tabular}
\renewcommand{\arraystretch}{1.0}
\end{minipage}
\end{table}

Tables~\ref{tab:comprehensive_metrics} and \ref{tab:computational_cost} present the complete evaluation results and the computational costs incurred per response while executing each configuration.

\begin{minipage}{0.48\textwidth}
\centering
\renewcommand{\arraystretch}{1.2}
\captionof{table}{Statistical Significance: Full System vs Baseline}
\label{tab:statistical_significance}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{p-value} & \textbf{Improve} \\
\midrule
PAS & 0.0037** & +19.9\% \\
BVS & 0.0000*** & +284.5\% \\
TRA & 0.0047** & +29.1\% \\
DEI & 0.0000*** & +100.0\% \\
CRRS & 0.0000*** & +102.6\% \\
\bottomrule
\multicolumn{3}{l}{** p < 0.01, *** p < 0.001} \\
\end{tabular}
\renewcommand{\arraystretch}{1.0}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{improvement_heatmap.png}
\captionof{figure}{ Performance gains from baseline}
\label{fig:improvement_heatmap}
\end{minipage}

Table~\ref{tab:statistical_significance} presents the statistical significance analysis and Figure~\ref{fig:improvement_heatmap} visualizes the performance improvements across all configurations.

\subsubsection{Metrics Analysis}

\textbf{Persona Adherence Score (PAS)}: Config4 achieves the highest individual PAS (0.661), demonstrating the Message Attributes Agent's effectiveness for persona consistency. The full system (Config5) shows a 19.9\% improvement over baseline.

\textbf{Behavioral Variance Score (BVS)}: Config5 achieves the most realistic behavioral variance (0.839), representing a 284.5\% improvement over the static baseline. Notably, Config4 shows zero variance, indicating over-rigid behavioral control without order state awareness.

\textbf{Task Restriction Adherence (TRA)}: Both Config3 and Config5 achieve equivalent high performance (0.785), confirming the State Tracking Agent's critical role in maintaining order accuracy.

\textbf{Decision Explainability Index (DEI)}: Only Config5 achieves near-perfect explainability (0.994) through comprehensive tool usage traces, while simpler configurations lack decision transparency.

\textbf{Composite Realism and Reliability Score (CRRS)}: Config5 demonstrates the highest overall performance (0.818), achieving a \textbf{102.6\% simulation quality improvement} over baseline and outperforming all partial configurations.


\subsubsection{Key Findings}

\textbf{Component Synergy}: Neither sub-agent alone (Config3, Config4) achieves optimal performance; the combination (Config5) creates synergistic effects.

\textbf{Behavioral Rigidity}: Pure behavioral control (Config4) without state awareness leads to robotic, templated persona-default style interactions (BVS = 0).

\textbf{Cost-Performance Trade-off}: The full system balances computational cost (14,789 tokens) with superior performance across all metrics.

\textbf{Statistical Significance}: All improvements in Config5 over baseline are statistically significant (p < 0.01), indicating the effectiveness of our multi-agent architecture across all metrics for simulating human user behaviour in a task oriented conversation.





