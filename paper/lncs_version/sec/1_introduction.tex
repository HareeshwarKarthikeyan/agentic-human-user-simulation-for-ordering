\section{Introduction}


The rapid deployment of conversational AI systems across diverse customer-facing applications from restaurant ordering and e-commerce to healthcare consultations and customer support~\cite{Rastogi2019SchemaGuidedDialogue,Sun2022MetaphoricalUserSimulators} has created an urgent need for comprehensive testing methodologies that can simulate realistic human user behavior~\cite{Balog2025UserSimulationEraGenerativeAI}. Current approaches rely on static test sets or human evaluators, both presenting significant limitations~\cite{Davidson2023UserSimulationLLM,Ahmad2025SimulatingUserDiversity}. Static tests fail to capture the dynamic, multi-turn nature of human conversations, while human evaluation is expensive, difficult to scale, and challenging to standardize across different interaction scenarios~\cite{Davidson2023UserSimulationLLM,Zhuge2024AgentAsAJudge}. Moreover, existing automated testing approaches typically lack the behavioral diversity and contextual awareness necessary to simulate realistic user interactions~\cite{Ahmad2025SimulatingUserDiversity,Park2025SimulatingHumanBehavior}. Traditional single-model approaches struggle to balance these requirements, producing either overly scripted interactions that fail to adapt, or unpredictable behaviors that compromise reliability and evaluation consistency~\cite{Chu2024CohesiveConversations,Castricato2024PERSONA,Mehri2025GoalAlignment,Wang2025SurveyLLMAgents}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Intro Figure.jpg}
\caption{\textbf{Comparison of simulation approaches:} \textbf{Left panel (Human User)}: Manual testing requires significant human effort including test scenario creation, multi-turn conversation management, person-in-role-playing, and result documentation, making it expensive and difficult to scale. \textbf{Center panel (Single Model System)}: Traditional automated approaches using a single model suffer from overloaded responsibilities, attempting to simultaneously handle state tracking and memory, behavior modeling, response generation, and context management, leading to inconsistent personas and poor interpretability. \textbf{Right panel (Agentic Simulation)}: Our proposed multi-agent framework distributes intelligence across specialized components, providing interpretable decisions, reproducible behavior, consistent personas, scalable architecture, and separated concerns, enabling systematic and reliable user simulation at scale. This decomposition mirrors human cognitive processes: tracking task completion progress (working memory)~\cite{Sun2022MetaphoricalUserSimulators,Hu2025UnifiedMindModel}, deciding how to respond based on personality and context (behavioral planning)~\cite{Park2023GenerativeAgents}, and generating appropriate utterances (language production).}
\label{fig:architecture}
\end{figure}


In this work, we propose a \textbf{multi-agent orchestration framework for human user simulation} in interactive scenarios that decomposes user behavior modeling into smaller, specialized components~\cite{Dang2025MultiAgentCollaboration,Lee2024OrchestraLLM}. Instead of relying on a single model, the framework employs distinct agents for managing task state, generating behavioral attributes, and coordinating interactions through structured protocols. To validate our approach, we implement and evaluate the framework in restaurant ordering - a domain that reflects the complexities of human interaction through multi-turn conversations, complex state tracking, and diverse persona-driven behaviors~\cite{Rastogi2019SchemaGuidedDialogue}. The framework rests on three core concepts: \textbf{Task State Management}, where a State Tracking Agent maintains a structured representation of the evolving task state, enabling precise progress tracking~\cite{Mehri2025GoalAlignment,Niu2024EnhancingDST}; \textbf{Behavioral Attribute Control}, where a Message Attributes Generation Agent dynamically determines conversational traits (mood, task execution style, exploration patterns) while preserving persona consistency~\cite{Ahmad2025SimulatingUserDiversity,WangChiu2023HumanoidAgents}; and \textbf{Tool-mediated Coordination}, where structured protocols govern agent interactions, ensuring proper context sharing without overlap of responsibilities~\cite{Raza2024TRiSM}. 

To our knowledge, this is the first work to explore explainable realistic human user simulation through a multi-agent architecture that combines dedicated agentic task tracking with fine-grained message generation attribute control. The unique internal environment we assess where specialized agents collaborate through structured protocols to maintain both task state coherence and persona consistency represents a novel approach in the user simulation landscape. This novelty informs our evaluation methodology, which focuses on demonstrating the framework's effectiveness through systematic ablation studies rather than direct comparisons with existing user simulation approaches that operate under fundamentally different architectural assumptions.

In summary, our work makes the following contributions:

\begin{itemize}
    \item[$\checkmark$] A novel multi-agent framework for human user simulation in interactive scenarios with specialized agents improving realism, controllability, and explainability through persona control and task state grounding
    
    \item[$\checkmark$] Systematic evaluation methodology with ablation studies and standardized metrics for persona adherence, task completion accuracy, decision explainability and overall simulation quality
    
    \item[$\checkmark$] Comprehensive test dataset in the restaurant ordering domain with 60 ordering test cases to validate the framework's effectiveness in complex, multi-turn conversational scenarios

\end{itemize}

